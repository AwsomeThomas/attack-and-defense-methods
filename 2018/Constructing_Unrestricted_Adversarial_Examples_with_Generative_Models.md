```
@inproceedings{DBLP:conf/nips/SongSKE18,
author = {Song, Yang and Shu, Rui and Kushman, Nate and Ermon, Stefano},
booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr{\'{e}}al, Canada.},
editor = {Bengio, Samy and Wallach, Hanna M and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicol{\`{o}} and Garnett, Roman},
pages = {8322--8333},
title = {{Constructing Unrestricted Adversarial Examples with Generative Models}},
url = {http://papers.nips.cc/paper/8052-constructing-unrestricted-adversarial-examples-with-generative-models},
year = {2018}
}
```
This paper proposed a new type of adversarial examples which are not constrained by adding imperceptible noise to exiting images, but generated by generative models from scratch. In their approach, they used GAN, where the conditional generator takes label as input and an auxiliary classifier is tnroduced to predict the label s of both training and generated images.

Specifically, they adapted WGAN with gradient penalty and Auxiliary Classifier GAN(AC-GAN) to stabilize training and use conditions. The objective functions can be found in their paper. And for adversarial loss, it consists of three parts: The first part is to encourage the targeted classifier to misclassify inputs; the second part soft-constrains the search region of randomly sampled noise vector, and according to the paper, the optimization may always converge to the sample example for each class without this constraint(but I do not figure out the reason); the third part is to encourage the auxiliary classifier to give correct preditions.

In addtion to this basic attack, they also proposed a noise-augmented attack, which added extra noise to the generated images. And the authors said *The representation power of the AC-GAN generator can be improved if we add small trainable noise to the generated image*.
