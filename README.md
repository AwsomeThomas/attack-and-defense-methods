# About
Inspired by [this repo](https://github.com/aleju/papers) and [ML Writing Month](https://docs.google.com/document/d/15o6m0I8g6O607mk5YPTh33Lu_aQYo7SpHhNSbLPQpWQ/mobilebasic?from=groupmessage#?utm_source=wechat_session&utm_medium=social&utm_oi=624560843380101120).  

# Papers
## Attack
- `ATTACK` `ICLR 2013` [Evasion Attacks against Machine Learning at Test Time](./2013/Evasion_attacks_against_machine_learning_at_test_time.md)
- `ATTACK` `ICLR 2014` [Intriguing properties of neural networks](./2014/Intriguing_properties_of_neural_networks.md)
- `ATTACK` `ICLR 2015` [Explaining and Harnessing Adversarial Examples](./2015/Explaining_and_Harnessing_Adversarial_Examples.md)
- `ATTACK` `EuroS&P 2016` [The limitations of deep learning in adversarial settings](./2016/The_limitations_of_deep_learning_in_adversarial_settings.md)
- `ATTACK` `CVPR 2016` [Deepfool](./2016/DeepFool.md)
- `ATTACK` `SP 2016` [C&W Towards evaluating the robustness of neural networks](./2016/Toward_evaluating_the_robustness_of_neural_networks.md)
- `Transferability` `ATTACK` `Arxiv 2016` [Transferability in machine learning: from phenomena to black-box attacks using adversarial samples](./2016/Transferability_in_machine_learning.md)
- `Transferability` `ATTACK` `CVPR 2019` [Feature Space Perturbations Yield More Transferable Adversarial Examples](./2019/Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples.md)
- `Transferability` `ATTACK` `ICLR 2017` [Delving into Transferable Adversarial Examples and Black-box Attacks](./2017/Delving_into_Transferable_Adversarial_Examples_and_Black-box_Attacks.md)
- `Adversarial Training` `ICLR 2019` [The Limitations of Adversarial Training and the Blind-Spot Attack](./2019/The_Limitations_of_Adversarial_Training_and_the_Blind-Spot_Attack.md)
- `ATTACK` `CVPR 2017` [Universal Adversarial Perturbations](./2017/Universal_Adversarial_Perturbations.md)
- `ATTACK` `ICLR 2018` [Generating Natural Adversarial Examples](./2018/Generating_Natural_Adversarial_Examples.md)
- `Theory` `ICLR 2019` [Are adversarial examples inevitable?](./2019/Are_adversarial_examples_inevitable.md) :thought_balloon:
- `Attack` `IEEE TEC 2019` [One pixel attack for fooling deep neural networks](./2019/One_pixel_attack_for_fooling_deep_neural_networks.md)
## Defense
- `Detection` `Arxiv 2017` [Detecting adversarial samples from artifacts](./2017/Detecting_Adversarial_Samples_from_Artifacts.md)
- `Detection` `ICLR 2017` [On Detecting Adversarial Perturbations](./2017/On_Detecting_Adversarial_Perturbations.md) :thought_balloon:

## GAN 
- `CVAE-GAN` `ICCV 2017` [CVAE-GAN: Fine-Grained Image Generation Through Asymmetric Training](./2017/CVAE-GAN_Fine-Grained_Image_Generation_Through_Asymmetric_Training.md)