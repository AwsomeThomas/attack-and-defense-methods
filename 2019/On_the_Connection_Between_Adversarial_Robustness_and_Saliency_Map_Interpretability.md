```
@inproceedings{DBLP:conf/icml/EtmannLMS19,
author = {Etmann, Christian and Lunz, Sebastian and Maass, Peter and Sch{\"{o}}nlieb, Carola},
booktitle = {Proceedings of the 36th International Conference on Machine Learning, {\{}ICML{\}} 2019, 9-15 June 2019, Long Beach, California, {\{}USA{\}}},
editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
pages = {1823--1832},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{On the Connection Between Adversarial Robustness and Saliency Map Interpretability}},
url = {http://proceedings.mlr.press/v97/etmann19a.html},
volume = {97},
year = {2019}
}
```

## Motivation
Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts.


we show that the interpretability of the saliency maps of a robustified neural network is not only a side-effect of adversarial training, but a general property enjoyed by networks with a high degree of robustness to adversarial perturbations.


We empirically demonstrate that the more linear the model is, the stronger the connection between robustness and alignment becomes.