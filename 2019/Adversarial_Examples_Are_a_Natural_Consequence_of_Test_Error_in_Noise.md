```
@inproceedings{DBLP:conf/icml/GilmerFCC19,
author = {Gilmer, Justin and Ford, Nicolas and Carlini, Nicholas and Cubuk, Ekin D},
booktitle = {Proceedings of the 36th International Conference on Machine Learning, {\{}ICML{\}} 2019, 9-15 June 2019, Long Beach, California, {\{}USA{\}}},
editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
pages = {2280--2289},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Adversarial Examples Are a Natural Consequence of Test Error in Noise}},
url = {http://proceedings.mlr.press/v97/gilmer19a.html},
volume = {97},
year = {2019}
}
```
## Motivation
Image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. **This paper tries to establishing close connections between the adversarial robustness and corruption robustness resaserch programs.**

Two types of errors:
- adversarial example resaerchers seek to measure and improve robustness to small-worst case perturbations of the input 
- corruption robustness researchers seek to measure and improve model robustness to distributional shift.



Overall, it's a paper I couldn't understand clearly.